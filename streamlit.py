# -*- coding: utf-8 -*-
"""StreamLit

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/173XFT3B8DwKxdOAWP0JGElPOW0PI0rLr
"""

# Single cell: installs deps, creates app.py (optional), launches Streamlit, tunnels via ngrok (optional), embeds in notebook.
import sys, subprocess, importlib.util, os, time, socket, textwrap
from IPython.display import IFrame, HTML, display

# -------------------- CONFIG --------------------
OVERWRITE_APP = True      # If True, will write the bundled app.py (replace if exists). Set False to keep existing app.py.
APP_FILE = "app.py"      # Streamlit app filename to create / run
PORT = 8501
USE_NGROK = False        # Set True if you want a public ngrok URL (requires NGROK_AUTH_TOKEN)
NGROK_AUTH_TOKEN = ""    # Paste your ngrok authtoken here if you want public tunneling; leave empty to skip or if USE_NGROK=False
STREAMLIT_TIMEOUT = 20      # seconds to wait for streamlit to start
# -------------------------------------------------

# -------------------- Helper: ensure packages --------------------
def ensure_pkg(pkg_name, import_name=None):
    if import_name is None:
        import_name = pkg_name
    spec = importlib.util.find_spec(import_name)
    if spec is None:
        print(f"Installing {pkg_name} ...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg_name], stdout=subprocess.DEVNULL)
    else:
        # print(f"{pkg_name} already installed.")  # keep output minimal
        pass

# ensure required packages
ensure_pkg("streamlit")
ensure_pkg("pyngrok")
ensure_pkg("psutil")

# import after ensuring
import psutil
from pyngrok import ngrok

# -------------------- Create a Streamlit app file (if requested) --------------------
app_code = r'''
# app.py - Resume Screener (minimal expanded version)
import streamlit as st
import pandas as pd
import numpy as np
import re
from collections import Counter
import plotly.express as px

# Layout
st.set_page_config(page_title="Resume Screener", layout="wide")
st.title("Resume Screener — Demo")

st.markdown("Upload a resume dataset (CSV/XLSX) and paste a Job Description to analyze candidates. This demo computes Overqualified, Education-to-Role Fit, and a simple shortlist heuristic.")

col1, col2 = st.columns([2,1])
with col1:
    uploaded = st.file_uploader("Upload resume dataset (CSV or Excel)", type=['csv','xlsx','xls'])
    jd_text = st.text_area("Job Description (paste full JD here)", height=200)
with col2:
    st.write("Options")
    preview_n = st.number_input("Rows to process (preview)", min_value=10, max_value=10000, value=500, step=10)

# Basic utilities (small subset of previous functions)
DEGREE_ORDER = ["highschool","diploma","bachelor","master","phd"]
DEGREE_MAP = {
    r"(high\s*school|12th|hsc|ssc|secondary)":"highschool",
    r"(diploma|polytechnic)":"diploma",
    r"(bachelor|b\.?tech|bsc|ba|bcom|bca)":"bachelor",
    r"(master|m\.?tech|msc|ma|mba|mca)":"master",
    r"(ph\.?d|doctorate)":"phd"
}
ROLE_ORDER = ["intern","junior","mid","senior","lead","manager","director","executive"]
ROLE_HINTS = {
    "intern":"intern|trainee",
    "junior":"junior|\\bjr\\b|associate",
    "mid":"analyst|engineer|consultant|specialist|associate",
    "senior":"senior|\\bsr\\b|staff|principal",
    "lead":"\\blead\\b|team lead|tech lead",
    "manager":"manager|\\bmgr\\b|head of",
    "director":"director|head\\b",
    "executive":"\\bvp\\b|vice president|chief|cxo|cto|cfo|ceo"
}

def normalize_degree(text):
    if not isinstance(text,str): return None
    t = text.lower()
    for pat,val in DEGREE_MAP.items():
        if re.search(pat, t): return val
    if "ph" in t and "d" in t: return "phd"
    if "mba" in t or "master" in t: return "master"
    if "bachelor" in t or "b." in t: return "bachelor"
    if "diploma" in t: return "diploma"
    if "school" in t: return "highschool"
    return None

def infer_role_level(title):
    if not isinstance(title,str): return None
    t = title.lower()
    for lvl in ROLE_ORDER[::-1]:
        pat = ROLE_HINTS.get(lvl)
        if pat and re.search(pat, t): return lvl
    if "senior" in t or " sr " in t: return "senior"
    if "manager" in t: return "manager"
    return "mid"

def extract_jd_requirements(text):
    if not isinstance(text,str): return 2,4,"bachelor","junior", set()
    txt = text.lower()
    m = re.search(r'(\d{1,2})\s*(?:[-–to]{1,3})\s*(\d{1,2})\s*years', txt)
    if m:
        jd_min, jd_max = int(m.group(1)), int(m.group(2))
    else:
        m2 = re.search(r'(\d{1,2})\s*\+\s*years', txt)
        if m2:
            jd_min = int(m2.group(1)); jd_max = jd_min + 2
        else:
            jd_min, jd_max = 2,4
    jd_degree = "bachelor"
    for pat,deg in DEGREE_MAP.items():
        if re.search(pat, txt):
            jd_degree = deg
            break
    jd_role = "junior"
    for lvl in ROLE_ORDER[::-1]:
        if re.search(r'\b'+re.escape(lvl)+r'\b', txt):
            jd_role = lvl
            break
    skills = set(re.findall(r'\b([a-z0-9\+\#]+)\b', txt))
    return jd_min, jd_max, jd_degree, jd_role, skills

def compute_overqualification(exp_years, role_level, edu_level, jd_min, jd_max, jd_role_idx, jd_edu_idx):
    reasons=[]
    over=False
    try:
        if exp_years is not None:
            ev=float(exp_years)
            if jd_max is not None and ev >= (jd_max+3):
                over=True; reasons.append(f"{ev:.1f} yrs vs JD {jd_min}-{jd_max}")
    except: pass
    try:
        if role_level is not None and jd_role_idx is not None:
            cand_idx = ROLE_ORDER.index(role_level) if role_level in ROLE_ORDER else None
            if cand_idx is not None and cand_idx - jd_role_idx >=2:
                over=True; reasons.append(f"title '{role_level}' above JD '{ROLE_ORDER[jd_role_idx]}'")
    except: pass
    try:
        if edu_level is not None and jd_edu_idx is not None:
            edu_idx = DEGREE_ORDER.index(edu_level) if edu_level in DEGREE_ORDER else None
            if edu_idx is not None and edu_idx - jd_edu_idx >=2:
                over=True; reasons.append(f"education '{edu_level}' >> JD '{DEGREE_ORDER[jd_edu_idx]}'")
    except: pass
    return over, "; ".join(reasons) if reasons else ""

def compute_education_fit(edu_level, jd_edu_idx):
    if jd_edu_idx is None: return "Unknown","No JD degree"
    if edu_level is None: return "Underqualified","No degree detected"
    edu_idx = DEGREE_ORDER.index(edu_level) if edu_level in DEGREE_ORDER else None
    if edu_idx is None: return "Unknown", f"Unrecognized {edu_level}"
    if edu_idx < jd_edu_idx: return "Underqualified", f"{edu_level} < required"
    if edu_idx == jd_edu_idx: return "Meets", f"{edu_level} meets requirement"
    return "Overqualified", f"{edu_level} > required"

# UI behavior
if uploaded is None:
    st.info("Upload a resume CSV/XLSX to get started.")
else:
    try:
        if uploaded.name.lower().endswith('.csv'):
            df = pd.read_csv(uploaded)
        else:
            df = pd.read_excel(uploaded)
    except Exception as e:
        st.error(f"Failed to read file: {e}")
        st.stop()

    st.success(f"Loaded {len(df)} rows.")
    st.write("Detected columns:", df.columns.tolist())

    jd_min, jd_max, jd_degree, jd_role, jd_skills = extract_jd_requirements(jd_text or "")
    jd_role_idx = ROLE_ORDER.index(jd_role) if jd_role in ROLE_ORDER else None
    jd_edu_idx = DEGREE_ORDER.index(jd_degree) if jd_degree in DEGREE_ORDER else None

    # Find likely column names
    def find_col(df, candidates):
        for c in candidates:
            if c in df.columns: return c
        lower_map = {col.lower():col for col in df.columns}
        for c in candidates:
            if c.lower() in lower_map: return lower_map[c.lower()]
        return None

    name_col = find_col(df, ['Name','Candidate','Full Name'])
    title_col = find_col(df, ['Job Role','Title','Designation','role'])
    edu_col = find_col(df, ['Education','Highest Education','Qualification','Degree'])
    exp_col = find_col(df, ['Experience (Years)','Experience','Years of Experience','yoe'])
    skills_col = find_col(df, ['Skills','Skillset','Key Skills','technical_skills'])
    projects_col = find_col(df, ['Projects Count','Projects','projects_count'])
    decision_col = find_col(df, ['Recruiter Decision', 'Decision'])
    salary_col = find_col(df, ['Salary Expectation ($)', 'Salary Expectation', 'Salary'])
    ai_score_col = find_col(df, ['AI Score (0-100)', 'AI Score'])


    results=[]
    rows = min(len(df), int(preview_n))
    for i, row in df.head(rows).iterrows():
        title = row.get(title_col) if title_col else None
        edu_raw = row.get(edu_col) if edu_col else None
        exp_raw = row.get(exp_col) if exp_col else None
        skills_raw = row.get(skills_col) if skills_col else ""
        proj_raw = row.get(projects_col) if projects_col else None

        edu_norm = normalize_degree(str(edu_raw)) if pd.notna(edu_raw) else None
        role_level = infer_role_level(str(title)) if pd.notna(title) else None
        try:
            exp_val = float(re.sub(r'[^\d\.]','', str(exp_raw))) if pd.notna(exp_raw) and str(exp_raw).strip()!="" else None
        except:
            exp_val = None
        skill_set = set([p.strip() for p in re.split(r'[;,/|\n]+', str(skills_raw).lower()) if p.strip()])
        try:
            proj_val = int(re.sub(r'[^\d]','', str(proj_raw))) if pd.notna(proj_raw) and str(proj_raw).strip()!="" else 0
        except:
            proj_val = 0

        is_over, over_reason = compute_overqualification(exp_val, role_level, edu_norm, jd_min, jd_max, jd_role_idx, jd_edu_idx)
        edu_fit_label, edu_fit_reason = compute_education_fit(edu_norm, jd_edu_idx)

        skill_match_ratio = 0.0
        if jd_skills:
            skill_match_ratio = len(jd_skills & skill_set) / max(1, len(jd_skills))

        shortlist = "No"
        if not is_over:
            edu_ok = (edu_fit_label != "Underqualified")
            exp_ok = (exp_val is None) or (exp_val >= (jd_min - 1))
            skills_ok = (skill_match_ratio >= 0.4) or (len(jd_skills)==0)
            if edu_ok and exp_ok and skills_ok:
                shortlist = "Yes"

        out = row.to_dict()
        out.update({
            "summary": f"{int(skill_match_ratio*100)}% JD skills; exp={exp_val}",
            "attrition_risk": "Unknown",
            "qualification_status": "Overqualified" if is_over else ("Good Fit" if edu_fit_label!="Underqualified" else "Underqualified"),
            "qualification_reason": over_reason or edu_fit_reason,
            "shortlist": shortlist,
            "Overqualified": "Yes" if is_over else "No",
            "Overqual_Reason": over_reason,
            "Edu_to_Role_Fit": edu_fit_label,
            "Edu_Fit_Reason": edu_fit_reason,
            "Skill_Match_Ratio": round(skill_match_ratio,3),
            "Projects_Count_Clean": proj_val
        })
        results.append(out)

    final_df = pd.DataFrame(results)

    st.subheader("Phase 1 — Enriched Data (preview)")
    st.dataframe(final_df, height=400)
    csv_bytes = final_df.to_csv(index=False).encode('utf-8')
    st.download_button("Download processed CSV (preview)", data=csv_bytes, file_name="processed_resume_preview.csv", mime="text/csv")

    st.subheader("Phase 2 — Charts & Key Findings")
    # education pie
    if 'Edu_to_Role_Fit' in final_df.columns:
        ec = final_df['Edu_to_Role_Fit'].fillna('Unknown').value_counts()
        fig = px.pie(values=ec.values, names=ec.index, title="Education-to-Role Fit", hole=0.35)
        st.plotly_chart(fig, use_container_width=True)
    # experience ranges
    if exp_col and exp_col in final_df.columns:
        exp_series = pd.to_numeric(final_df[exp_col], errors='coerce').fillna(-1)
        bins=[-1,2,5,10,999]
        labels=['0-2','3-5','6-10','10+']
        exp_cat = pd.cut(exp_series, bins=bins, labels=labels)
        counts = exp_cat.value_counts().reindex(labels).fillna(0)
        fig2 = px.bar(x=counts.index, y=counts.values, title="Experience Ranges")
        st.plotly_chart(fig2, use_container_width=True)
    # top skills
    if skills_col and skills_col in final_df.columns:
        all_sk = final_df[skills_col].fillna('').astype(str).tolist()
        skill_list=[]
        for s in all_sk:
            parts = re.split(r'[;,/|\n]+', s.lower())
            for p in parts:
                p=p.strip()
                if p:
                    skill_list.append(p)
        if skill_list:
            from collections import Counter
            top = Counter(skill_list).most_common(15)
            sk_names=[t[0] for t in top][::-1]
            sk_vals=[t[1] for t in top][::-1]
            fig3 = px.bar(x=sk_vals, y=sk_names, orientation='h', title="Top Skills (Top 15)")
            st.plotly_chart(fig3, use_container_width=True)

    st.markdown("---")
    st.subheader("Deeper Analysis of Recruiter Decisions")

    if decision_col and decision_col in final_df.columns:
        hired_df = final_df[final_df[decision_col] == 'Hire']
        rejected_df = final_df[final_df[decision_col] == 'Reject']

        # Hired vs Rejected Dashboard
        st.markdown("#### Hired vs. Rejected Dashboard")
        if not hired_df.empty or not rejected_df.empty:
            avg_metrics = {
                'Decision': ['Hired', 'Rejected'],
                'Average AI Score': [
                    hired_df[ai_score_col].mean() if ai_score_col and not hired_df.empty else 0,
                    rejected_df[ai_score_col].mean() if ai_score_col and not rejected_df.empty else 0
                ],
                'Average Experience (Yrs)': [
                    hired_df[exp_col].mean() if exp_col and not hired_df.empty else 0,
                    rejected_df[exp_col].mean() if exp_col and not rejected_df.empty else 0
                ],
                'Average Projects Count': [
                    hired_df[projects_col].mean() if projects_col and not hired_df.empty else 0,
                    rejected_df[projects_col].mean() if projects_col and not rejected_df.empty else 0
                ]
            }
            avg_df = pd.DataFrame(avg_metrics)

            col1, col2 = st.columns(2)
            with col1:
                 fig_decision_metrics = px.bar(avg_df, x='Decision', y=['Average AI Score', 'Average Experience (Yrs)', 'Average Projects Count'],
                                          barmode='group', title="Key Metrics: Hired vs. Rejected")
                 st.plotly_chart(fig_decision_metrics, use_container_width=True)

            with col2:
                if edu_col and not hired_df.empty:
                    edu_counts = hired_df[edu_col].value_counts()
                    fig_edu_donut = px.pie(values=edu_counts.values, names=edu_counts.index, title="Education Level of Hired Candidates", hole=0.4)
                    st.plotly_chart(fig_edu_donut, use_container_width=True)

        # Top Skills of Hired Candidates
        st.markdown("#### Top Skills of Hired Candidates")
        if skills_col and not hired_df.empty:
            hired_skills = hired_df[skills_col].dropna().astype(str).tolist()
            hired_skill_list = []
            for s in hired_skills:
                parts = re.split(r'[;,/|\n]+', s.lower())
                for p in parts:
                    p = p.strip()
                    if p:
                        hired_skill_list.append(p)

            if hired_skill_list:
                top_hired = Counter(hired_skill_list).most_common(15)
                sk_names_hired = [t[0] for t in top_hired][::-1]
                sk_vals_hired = [t[1] for t in top_hired][::-1]
                fig_hired_skills = px.bar(x=sk_vals_hired, y=sk_names_hired, orientation='h', title="Top 15 Skills (Hired Candidates)")
                st.plotly_chart(fig_hired_skills, use_container_width=True)
    else:
        st.warning("Could not find a 'Recruiter Decision' column for analysis.")


    st.markdown("---")
    st.subheader("Salary Expectation Insights")

    if salary_col and title_col and salary_col in final_df.columns and title_col in final_df.columns:
        # Clean salary data
        final_df[salary_col] = pd.to_numeric(final_df[salary_col], errors='coerce')
        salary_df = final_df.dropna(subset=[salary_col])

        # Salary Distribution
        st.markdown("#### Salary Expectation Distribution")
        fig_salary_dist = px.histogram(salary_df, x=salary_col, title="Distribution of Salary Expectations", nbins=50)
        st.plotly_chart(fig_salary_dist, use_container_width=True)

        # Salary by Job Role
        st.markdown("#### Salary Expectation by Job Role")
        fig_salary_role = px.box(salary_df, x=title_col, y=salary_col, title="Salary vs. Job Role", points="all")
        fig_salary_role.update_layout(xaxis_title="Job Role", yaxis_title="Salary Expectation ($)")
        st.plotly_chart(fig_salary_role, use_container_width=True)

        # Salary vs. Experience
        if exp_col and exp_col in final_df.columns:
            st.markdown("#### Salary Expectation vs. Experience")
            exp_salary_df = final_df.dropna(subset=[salary_col, exp_col])
            fig_salary_exp = px.scatter(exp_salary_df, x=exp_col, y=salary_col, color=title_col,
                                        title="Salary vs. Experience", trendline="ols",
                                        labels={exp_col: "Experience (Years)", salary_col: "Salary Expectation ($)"})
            st.plotly_chart(fig_salary_exp, use_container_width=True)

    else:
        st.warning("Could not find 'Salary Expectation ($)' or 'Job Role' columns for analysis.")

    st.markdown("---")
    st.write("Note: LLM integration is disabled in this demo. To enable LLM scoring, integrate your LLM client and add API handling.")
'''

if OVERWRITE_APP or (not os.path.exists(APP_FILE)):
    with open(APP_FILE, "w", encoding="utf-8") as f:
        f.write(app_code)
    print(f"Written {APP_FILE} (OVERWRITE_APP={OVERWRITE_APP}).")
else:
    print(f"{APP_FILE} exists and OVERWRITE_APP=False — leaving it untouched.")

# -------------------- Start Streamlit app as background process --------------------
import subprocess, psutil, signal

# kill any previous streamlit processes started with the same app (best-effort)
def kill_existing_streamlit(app_name=APP_FILE):
    for p in psutil.process_iter(['pid','name','cmdline']):
        try:
            cmd = p.info.get('cmdline') or []
            if cmd and 'streamlit' in " ".join(cmd) and app_name in " ".join(cmd):
                print("Killing previous process:", p.pid, cmd)
                p.kill()
        except Exception:
            pass

kill_existing_streamlit(APP_FILE)

STREAMLIT_CMD = [sys.executable, "-m", "streamlit", "run", APP_FILE, "--server.headless", "true", "--server.port", str(PORT)]
print("Launching Streamlit with command:", " ".join(STREAMLIT_CMD))
proc = subprocess.Popen(STREAMLIT_CMD, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, preexec_fn=os.setsid if os.name!="nt" else None)

# wait for port to be open
def wait_for_port(port, timeout=STREAMLIT_TIMEOUT):
    start = time.time()
    while time.time() - start < timeout:
        s = socket.socket()
        try:
            s.connect(("127.0.0.1", port)); s.close(); return True
        except Exception:
            time.sleep(0.5)
    return False

if not wait_for_port(PORT, timeout=STREAMLIT_TIMEOUT):
    print(f"Warning: Streamlit did not open port {PORT} in {STREAMLIT_TIMEOUT}s. Showing recent stderr:")
    try:
        err = proc.stderr.read()
        print(err[-2000:])
    except Exception as e:
        print("Could not read stderr:", e)

# -------------------- Create ngrok tunnel if requested --------------------
public_url = None
if USE_NGROK:
    if not NGROK_AUTH_TOKEN:
        print("NGROK_AUTH_TOKEN empty — cannot create ngrok tunnel. Set NGROK_AUTH_TOKEN and rerun or set USE_NGROK=False.")
    else:
        try:
            ngrok.set_auth_token(NGROK_AUTH_TOKEN)
            tunnel = ngrok.connect(addr=PORT, proto="http")
            public_url = str(tunnel)
            print("Ngrok public URL:", public_url)
        except Exception as e:
            print("Ngrok error:", e)
            public_url = None

# -------------------- Embed app in notebook --------------------
if public_url:
    url = public_url if public_url.startswith("http") else f"http://{public_url}"
    display(HTML(f'<b>Streamlit app (ngrok):</b> <a href="{url}" target="_blank">{url}</a>'))
    display(IFrame(src=url, width="100%", height=700))
else:
    local_url = f"http://localhost:{PORT}"
    print("Opening local URL:", local_url)
    display(HTML(f'<b>Streamlit app (local):</b> <a href="{local_url}" target="_blank">{local_url}</a>'))
    display(IFrame(src=local_url, width="100%", height=700))

# -------------------- Cleanup helper --------------------
def cleanup():
    # kill ngrok tunnels
    try:
        ngrok.kill()
    except Exception:
        pass
    # kill streamlit processes for this app
    for p in psutil.process_iter(['pid','name','cmdline']):
        try:
            cmd = p.info.get('cmdline') or []
            if cmd and 'streamlit' in " ".join(cmd) and APP_FILE in " ".join(cmd):
                print("Killing streamlit process", p.pid)
                p.kill()
        except Exception:
            pass
    print("Cleanup attempted. If streamlit process persists, restart kernel or kill manually.")
# expose cleanup to user namespace
globals()['cleanup'] = cleanup

print("Done — use cleanup() to stop the app and close tunnels.")